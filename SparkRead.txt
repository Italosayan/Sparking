Spark_book

Spark Core consists on 2 APIs. Unstructured and Structured.

Unstructured(Raw Objects):
Resilient Distributed Datasets(RRDs)
Accumulators
Broadcast Variables

Structured(Most used):
DataFrames
Datasets
Spark SQL

Outside Spark Core:
MLlib
GrapX:Graph Processing
SparkR

----o----

Spark Applications have a driver(D) and executor(E) processes.

D:
Maintains information 
Responds to User
Distributes and Schedules work to Es.

E:
Executes Code
Reporting back to D

Cluster manager(YARN Mesos):
Allocates resources
Controls physical machines

Starting Spark
./bin/spark-shell on spark directory

SparkSession <- Driver
called spark on console

Creating a column on driver:
val myRange = spark.range(1000).toDF("number")
number is colname
It's a DF with one column containing 1000 rows with values from 0 to 999.
No list of numbers only a type signature.
(Distributed Collection)

DataFrames:
The list of columns and types are a schema. Just like in SQL.
A SparkDataFrame can span potentially thousands of computers.
DataFrame is the simplest distributed collection.

Characteristics of all Datasets,dataframes, and RDDs
	Partitions:
	Chunck of data on a physical machine.
	errand - computation and data
	(in DF you dont manipulate partitions)

	Transformations:
	Change a DF to the one you want.
	For example: val divisby2 = myRange.where("number % 2 = 0")
	No computation is executed
	
	Lazy Evaluation:
	Build a plan of transformations
	
	Actions:
	.count() on divisby2 returns 500. Some computation
	.collect()
	Three kinds: view data in console, collect that in R|Python|Scala, and write to output data sources

	Spark has a UI: localhost:4040
	A Spark job is a set of transformations triggered by an individual action.

A Basic Transformation Data Flow
val flightData2015 = spark.read.json("databook/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json")
val flightData2015 = spark.read.json(PATH) One way to read
flightData2015.take(2)
flightData2015.explain() <- Physical Plan (Just read in this case)
flightData2015.show() <- to see data
lineage(?) ancestors and transformations
val flightData2015 = spark.read.option("inferSchema","true").option("header","true").csv(PATH)

Dataframes and SQL
To create a view:
flightData2015.createOrReplaceTempView("flight_data_2015")

Using SQL:
val sqlWay = spark.sql(""" SELECT DEST_COUNTRY_NAME, count(1) FROM flightData2015 GROUP BY DEST_COUNTRY_NAME """)
Using spark:
val DataFrameWay = flightData2015.groupBy('DEST_COUNTRY_NAME').count()
Same thing under the hood.

Maximun flight

import org.apache.spark.sql.functions.max
flightData2015.select(max("count")).take(1)
SQL: spark.sql("SELECT max(count) from flight_data_2015").take(1)

More complex:
flightData2015
.groupBy("DEST_COUNTRY_NAME")
.sum("count")
.withColumnRenamed("sum(count)", "destination_total")
.sort(desc("destination_total"))
.limit(5)
.collect()

CHAPTER 02!!Structured API-> no classes. Tables or dataframes

Typed and Untyped API
Dataframe or untyped API goes accross prog. languages. AKA spark types. no native types. Sub of typed
Dataset API or typed API only works in JVM. Scala and java. Define your own types. Types are checked at compile time
spark.range(500) --> org.apache.spark.sql.Dataset[Long] = [id: bigint]
Each record of a dataFrame is of type Row
TYPE TABLE
Columns represent types
spark.range(10).collect() --> native scala(encoders)
spark.range(10).toDF().collect() --> spark type

Overview of Spark Execution
1. Write DataFrame/Dataset/SQL Code
2. If valid code, SPark converts this to a Logical Plan(using catalyst optimizer)
3. Spark transforms to physical plan(taking DataFrames and SQL and complites them to RDD. Creates different execution strategies and chooses.
4. excecutes plan on clusters.  At runtime, Java bytecode is generate to remove whole tasks.
Catalyst Optimizer creates the plan (CRAZY)
Pushes down predicates and selections


CHAPTER 03!! Basic Structured Operations
The partitioning of the DataFrame define the physical distribution across clusters
Partitioning scheme defines if its on values in a certain column orn non deterministically
A schema is a StructType made of StructFields
val myManualSchema = new StructType(Array(
	new StructField("DEST_COUNTRY_NAME", StringType, true),
	new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
	new StructField("count", LongType, false)
WE CANNOT SIMPLY SET TYPES VIA THE PER LANGUAGE TYPE BECAUSE SPARK MAINTAINS ITS OWN TYPE INFO

To Spark a column just represents a logical(?) value computed per row though an expression
To create a column

col("COLNAME")
Columns are not resolved till we compare a name with those we are maintaining in the catalog
df.col("COLNAME") (no need to be resolved on analyzer phase)
EXPRESSION: set of transformations. like a function that takes COLNAMES resolves them to create a single value
example of expression: expr('(((someCol + 5 )*200)-6)< otherCol" ')
Create logic tree similar to SQL. SEE IMAGE IN BOOK
You have to set the types in scala

there are 4 types of dataframe transformations
We can add rows or columns
We can remove rows or columns
We can transform a row into a column
We can change the order of rows based on the values in columns

Creating DF
remember to createOrReplaceTempView("dfTable")
import Row
import Types

val myManualSchema = new StructType(Array(
	new StructField("some", StringType, true),
	new StructField("col", StringType, true),
	new StructField("names", LongType, false)))

val myRows = Seq(Row("Hello", null, 1L))
val myRDD = spark.sparkContext.parallelize(myRows)

val myDF = spark.createDataFrame(myRDD, myManualSchema)
myDF.show()

Two Useful Methods
select->columns
df.select("colname","colname2").show(2)
selectExpr->strings "DEST_COUNTRY_NAME as newname"
df.selectExpr("*", "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry")

Average of count column. Distinct destinations
flightData2015.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show()
more functions on th org.apache.spark.sql.functions

Passing a value to compare to
imoprt org.apache.spark.sql.functions.lit
df.select(expr("*"), lit(1).as("something")).show(2)

Adding Columns
df.withColumn("numberOne",lit(1)).show(2)
df.withColumn("same", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show(2)
Rename: df.WithColumn("Destination",df.col("DEST_COUNTRY_NAME")).columns
Other rename: df.withColumnRenamed("DEST_COUNTRY_NAME","dest"),columns
`` have to be used when refering to a column in an expression

Drop: df.drop("ORIGIN_COUNTRY_NAME").columns
dfWithLongColName.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")
Casting:
df.withColumn("count",col("count").cast("int")).printSchema()
filtering columns:
df.where(col("count")<2)
.where(col("ORIGIN_COUNTRY_NAME") =!= "Croatia").show(2)
=!= distinct to

To get unique rows
df.select("colname").distinct().count()

Function appendix:
Cool Spark stuff:
$"myColumn"
'myColumn
./bin/spark-shell
Transformations
	spark.read.json(PATH) One way
	val myRange = spark.range(1000).toDF("number")
	val divisby2 = myRange.where("number % 2 = 0")
	.groupBy('DEST_COUNTRY_NAME')
	.withColumnRenamed("old","new")
	.sort(desc('col1'))
	.limit()
	.join()
	vak myRow = Row("Hello", null, 1, flase)
Actions
	.count() on divisby2 returns 500
	.collect() - Return all values of DF
	.show() - like str in R
	flightData2015.select(max("number_of_flights")).take(1)
	.take(2) - like head
	.select(col1,col2)
	.firt()
Extra
	flightData2015.explain( ) -Physical Plan
	flightData2015.createOrReplaceTempView("flight_data_2015") -Create SQL view
	flightData2015.printSchema()
	df.schema()
	SPARK UI: localhost:4040
	.columns to see al columns
